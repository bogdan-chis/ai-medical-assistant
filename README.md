# Dr. Dialogue ğŸ¤–ğŸ¥

> **A medicalâ€domain conversational assistant** powered by retrievalâ€augmented generation (RAG) over a curated doctorâ€“patient dialogue dataset, with answer evaluation via a secondary LLM. ğŸ’¬ğŸ“š

---

## ğŸ§ Project Overview

**Dr. Dialogue**leverages a locally hosted LLM (via LM Studio) and a doctorâ€“patient conversation dataset to answer user (patient) queries with contextually relevant, upâ€toâ€date medical information. All answers are then run through a secondary â€œDeepEvalâ€ LLM to ensure consistency, accuracy, and safety before returning to the user (also locally hosted). ğŸ”’âœ…

---

## âœ¨ Features

* **ğŸ” Retrievalâ€Augmented Generation (RAG):**

  * Ingests and preprocesses raw doctorâ€“patient transcripts.
  * Splits and embeds each chunk with a local embeddings service.
  * Stores embeddings in ChromaDB for fast nearestâ€neighbor lookups.
  * Retrieves the most relevant snippets at query time and supplies them as context to the LLM.

* **ğŸ¥ Local LLM Hosting:**

  * **Primary LLM** (e.g., a fineâ€tuned medical model) runs on a local LM Studio instance.
  * **Secondary LLM (DeepEval)** also hosted locally to evaluate clarity, quality, and safety of each generated response.

* **âš™ï¸ FastAPI Backend:**

  * Exposes REST endpoints for ingestion, querying, and evaluation.
  * Manages ingestion triggers, embedding calls, RAG orchestration, and evaluation pipelines.

* **ğŸ–¥ï¸ React Frontend:**

  * Provides a simple chat interface for endâ€users (patients).
  * Sends user queries over HTTP, receives and displays filtered, LLMâ€generated medical guidance.

* **ğŸ›¡ï¸ Safety & Guardrails:**
  
  * Maintains custom promptâ€engineering templates and guardrails for medical disclaimers.

---

## ğŸš€ Getting Started

Follow these steps to get **Dr. Dialogue** up and running locally.

### 1. Prerequisites

* **Python 3.8+**
* **Node.js & npm** (v14+)
* **LM Studio** installed and licensed locally

### 2. Clone the Repository

```bash
git clone https://github.com/bogdan-chis/ai-medical-assistant.git
```

### 3. Configure Environment Variables

Copy the example environment file and update necessary values:

```bash
cp .env.example .env
```

Open `.env` and set the following:

```dotenv
# --- existing RAG LLM (phi-4) ---
LLM_API_BASE_URL=http://localhost:1234/v1
LLM_API_KEY=lm-studio
LLM_MAIN_MODEL=phi-4

# --- new Judge LLM (locally-hosted) ---
JUDGE_API_BASE_URL=http://localhost:1234/v1
JUDGE_API_KEY=lm-studio
JUDGE_MODEL_PATH=phi-4
```

You can modify the LM Studio URL and both API keys here at any time.

### 4. Install Dependencies

#### Backend

```bash
pip install -r requirements.txt
```

#### Frontend

```bash
cd frontend
npm install
cd ..
```

### 5. Start the LM Studio Server

Launch your local LM Studio instance per its documentation. Once running, confirm the URL and keys match those in your `.env` file.

### 6. Run the Application

#### a. Start the Python Backend Server

From the project root:

```bash
python -m backend.api.main
```

The FastAPI server will start (default `http://127.0.0.1:8000`).

#### b. Launch the React Frontend

In a separate terminal:

```bash
cd frontend
npm.cmd start
```

This will open the web UI (usually at `http://localhost:3000`).

---

## ğŸ—ï¸ Architecture

```text
+-------------------------------------------------------------+
|                         Frontend Layer                      |
|  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  |
|  | React UI  <â”€â”€â”€ HTTP: Query/Response â”€â”€  HTTP Wrappers |  |
|  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  |
|                              â”‚                              |
|                              â–¼                              |
|                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   |
|                     â”‚  API Server       â”‚                   |
|                     â”‚  (FastAPI)         â”‚â—€â”€â”              |
|                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚              |
+-------------------------------------------------------------+
          â”‚            â–²                      â”‚
          â”‚            â”‚                      â”‚
          â”‚         Trigger                   â”‚
          â”‚         Ingestion                 â”‚
          â–¼            â”‚                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚       Ingestion & Preprocessing     â”‚       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚       â”‚
â”‚  â”‚Dataset      â”‚â”€â”€â–¶ â”‚Document   â”‚â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  â”‚Loader       â”‚    â”‚Splitter   â”‚â—€â”€â”˜
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚         â”‚                  â”‚
â”‚         â–¼                  â–¼
â”‚   Postprocessing        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         â”‚               â”‚ Embeddings   â”‚
â”‚         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Service      â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       Query Embeddings          â”‚
                                 â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚Vector Store   â”‚
                          â”‚Interface      â”‚â”€â”€â”€â”
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                                  â”‚           â”‚
                                  â–¼           â”‚
                           Retrieve Context   â”‚
                                  â”‚           â”‚
                                  â–¼           â”‚
                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
                             â”‚   RAG     â”‚â”€â”€â”€â”€â”˜
                             â”‚ Service   â”‚
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚                         â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ LLM      â”‚             â”‚ DeepEval   â”‚
                â”‚ Client   â”‚             â”‚ Client     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚                         â”‚
                     â”‚                         â”‚
             (Response from LLM)        (Evaluation Score)
                     â”‚                         â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â–¼
                            Final Response
```
